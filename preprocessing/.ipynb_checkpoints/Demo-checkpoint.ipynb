{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e1d91e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages\n",
      "Requirement already satisfied: six in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from rouge)\n",
      "\u001b[33mYou are using pip version 20.3.4, however version 21.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: rouge_metric in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages\n",
      "\u001b[33mYou are using pip version 20.3.4, however version 21.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting newspaper3k\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
      "\u001b[K    100% |████████████████████████████████| 215kB 1.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.2.1 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from newspaper3k)\n",
      "Collecting Pillow>=3.3.0 (from newspaper3k)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/56/3d/f3031fe0a88b797fd09ee0772f611e65be34f30263eefb838cce8f367e75/Pillow-7.2.0-cp35-cp35m-manylinux1_x86_64.whl (2.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.2MB 142kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting feedfinder2>=0.0.4 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
      "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.4MB 45kB/s  eta 0:00:01    97% |███████████████████████████████▎| 7.2MB 76.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=3.11 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from newspaper3k)\n",
      "Collecting lxml>=3.6.0 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/0a/12/ec66192d5eca20a92302af5a6ce6147d96dc4b65050e9b0509180adf6252/lxml-4.6.3-cp35-cp35m-manylinux1_x86_64.whl (5.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.5MB 59kB/s  eta 0:00:01    21% |██████▉                         | 1.2MB 38.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from newspaper3k)\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/88/99/503196ca5e9e69a44d1b3e0e1bc0ca2a2fa0c12b09041b14b52bb3ef3523/feedparser-6.0.0-py2.py3-none-any.whl (82kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 3.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tinysegmenter==0.3 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
      "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/7e/62/b6acd3129c5615b9860e670df07fd55b76175b63e6b7f68282c7cad38e9e/tldextract-3.1.0-py2.py3-none-any.whl (87kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 1.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cssselect>=0.9.2 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
      "Collecting beautifulsoup4>=4.4.1 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 3.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from newspaper3k)\n",
      "Requirement already satisfied: regex in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from nltk>=3.2.1->newspaper3k)\n",
      "Requirement already satisfied: tqdm in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from nltk>=3.2.1->newspaper3k)\n",
      "Requirement already satisfied: joblib in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from nltk>=3.2.1->newspaper3k)\n",
      "Requirement already satisfied: click in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from nltk>=3.2.1->newspaper3k)\n",
      "Requirement already satisfied: six in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from feedfinder2>=0.0.4->newspaper3k)\n",
      "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
      "Requirement already satisfied: idna in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from tldextract>=2.0.1->newspaper3k)\n",
      "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
      "Collecting filelock>=3.0.8 (from tldextract>=2.0.1->newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/93/83/71a2ee6158bb9f39a90c0dea1637f81d5eef866e188e1971a1b1ab01a35a/filelock-3.0.12-py3-none-any.whl\n",
      "Collecting soupsieve>1.2; python_version >= \"3.0\" (from beautifulsoup4>=4.4.1->newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/02/fb/1c65691a9aeb7bd6ac2aa505b84cb8b49ac29c976411c6ab3659425e045f/soupsieve-2.1-py3-none-any.whl\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from requests>=2.10.0->newspaper3k)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from requests>=2.10.0->newspaper3k)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from requests>=2.10.0->newspaper3k)\n",
      "Building wheels for collected packages: feedfinder2, jieba3k, tinysegmenter, sgmllib3k\n",
      "  Running setup.py bdist_wheel for feedfinder2 ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/duvvuri.s/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
      "  Running setup.py bdist_wheel for jieba3k ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/duvvuri.s/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
      "  Running setup.py bdist_wheel for tinysegmenter ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/duvvuri.s/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
      "  Running setup.py bdist_wheel for sgmllib3k ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/duvvuri.s/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
      "Successfully built feedfinder2 jieba3k tinysegmenter sgmllib3k\n",
      "Installing collected packages: Pillow, soupsieve, beautifulsoup4, feedfinder2, jieba3k, lxml, sgmllib3k, feedparser, tinysegmenter, requests-file, filelock, tldextract, cssselect, newspaper3k\n",
      "Successfully installed Pillow-7.2.0 beautifulsoup4-4.9.3 cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.0 filelock-3.0.12 jieba3k-0.35.1 lxml-4.6.3 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 soupsieve-2.1 tinysegmenter-0.3 tldextract-3.1.0\n",
      "\u001b[33mYou are using pip version 20.3.4, however version 21.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge\n",
    "!pip install rouge_metric\n",
    "!pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c1029d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: feedparser==5.2.1 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages\n",
      "\u001b[33mYou are using pip version 20.3.4, however version 21.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting pandas\n",
      "  Downloading https://files.pythonhosted.org/packages/a9/55/e3f34ad611f703454b951bab6bde9a432f1af92994cebc4d8e0ec0af38c4/pandas-0.25.3-cp35-cp35m-manylinux1_x86_64.whl (10.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 10.3MB 31kB/s  eta 0:00:01  7% |██▎                             | 747kB 7.9MB/s eta 0:00:02    16% |█████▏                          | 1.7MB 10.5MB/s eta 0:00:01    32% |██████████▍                     | 3.3MB 54.2MB/s eta 0:00:01    97% |███████████████████████████████ | 10.0MB 87.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from pandas)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from pandas)\n",
      "Collecting pytz>=2017.2 (from pandas)\n",
      "  Downloading https://files.pythonhosted.org/packages/70/94/784178ca5dd892a98f113cdd923372024dc04b8d40abe77ca76b5fb90ca6/pytz-2021.1-py2.py3-none-any.whl (510kB)\n",
      "\u001b[K    100% |████████████████████████████████| 512kB 463kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from python-dateutil>=2.6.1->pandas)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-0.25.3 pytz-2021.1\n",
      "\u001b[33mYou are using pip version 20.3.4, however version 21.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install feedparser==5.2.1\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "abc0a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from word_embedding import embed_sentences\n",
    "from dataload import loadTestData \n",
    "#from rouge import Rouge\n",
    "import joblib\n",
    "from rouge_metric import PyRouge\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "from newspaper import Article\n",
    "from newspaper.article import ArticleDownloadState, ArticleException\n",
    "from time import sleep\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48104a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_loadTestData():\n",
    "    testing_data = [ [ np.array([\"This sentence is important for doc0.\" ,\n",
    "                                 \"Such a sentence is irrelevent for doc 0.\"]), \n",
    "                       np.random.rand(2,5,300), \n",
    "                       np.array([\"This sentence is important for doc0.\"]) ],\n",
    "                     [ np.array([\"Lol that sentence is awesome for do1.\" , \n",
    "                                 \"No way, this is irrelevent\"]), \n",
    "                       np.random.rand(2,5,300), \n",
    "                                np.array([\"Lol that sentence is awesome for do1.\"]) ] ]\n",
    "    return testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f7d942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_temp = dummy_loadTestData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e913f6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['Lol that sentence is awesome for do1.',\n",
       "        'No way, this is irrelevent'], dtype='<U37'),\n",
       " array([[[0.13952658, 0.57798297, 0.71356846, ..., 0.09727206,\n",
       "          0.08339716, 0.42132056],\n",
       "         [0.97713175, 0.20028658, 0.27412616, ..., 0.19625067,\n",
       "          0.3273923 , 0.0762757 ],\n",
       "         [0.22154281, 0.07019896, 0.2722557 , ..., 0.42949362,\n",
       "          0.18148447, 0.3250534 ],\n",
       "         [0.90920583, 0.17384241, 0.1027518 , ..., 0.00801292,\n",
       "          0.61239681, 0.50359767],\n",
       "         [0.69925901, 0.66450187, 0.43771454, ..., 0.28400876,\n",
       "          0.31714473, 0.79993739]],\n",
       " \n",
       "        [[0.71283644, 0.42956767, 0.09803115, ..., 0.78058513,\n",
       "          0.84218254, 0.42831162],\n",
       "         [0.26065473, 0.38980305, 0.82476535, ..., 0.53272599,\n",
       "          0.83900447, 0.93285412],\n",
       "         [0.18889732, 0.43206335, 0.10429396, ..., 0.26024677,\n",
       "          0.09137992, 0.29904015],\n",
       "         [0.22707177, 0.84977624, 0.94196255, ..., 0.35684511,\n",
       "          0.31490146, 0.89682339],\n",
       "         [0.33627537, 0.46809441, 0.6881238 , ..., 0.25266897,\n",
       "          0.53730408, 0.19552945]]]),\n",
       " array(['Lol that sentence is awesome for do1.'], dtype='<U37')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_temp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "03e5cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, testing_data, batch_size = 128, upper_bound = 100, threshold = 1):\n",
    "    \"\"\"\n",
    "        Build the actual summaries for test data and evaluate them\n",
    "        To do: \n",
    "            - load the actual x_test (embed test sentences) and y_test (compute rouge score)\n",
    "        \n",
    "        Parameters: \n",
    "            testing_data           - np.array \n",
    "                                        ex: [ doc1, doc2, ... , docn]\n",
    "                                         where doci = [sentences, x_test, summary]\n",
    "                                             where sentences = np.array of string\n",
    "                                                   x_test = np.array of matrices (embedded sentences)\n",
    "                                                   summaries = np.array of sentences\n",
    "        \n",
    "        Returns: \n",
    "            Rouge evaluations\n",
    "    \"\"\"   \n",
    "    #rouge = Rouge()\n",
    "    rouge = PyRouge(rouge_n=(1, 2, 4), rouge_l=True, rouge_w=True,\n",
    "                rouge_w_weight=1.2, rouge_s=True, rouge_su=True, skip_gap=4)\n",
    "    r1evals = []\n",
    "    r2evals = []\n",
    "    summaries = []    \n",
    "\n",
    "    all_predicted_summary = []\n",
    "    all_true_summary = []\n",
    "    \n",
    "    \n",
    "    for doc in testing_data: \n",
    "        sentences = doc[0]\n",
    "        \n",
    "        x_test_old = doc[1]\n",
    "        s1 = x_test_old.shape[0]\n",
    "        (s3,s4) = x_test_old[0].shape\n",
    "        print(s1,s3,s4)\n",
    "        x_test = np.random.rand(s1,1,190,s4)\n",
    "        for i in range(s1) :\n",
    "            x_test[i] = np.array( [ np.pad(x_test_old[i], ((190-s3,0),(0,0)), 'constant') ] )\n",
    "            \n",
    "\n",
    "        true_summary = doc[2]\n",
    "        \n",
    "        predicted_scores = model.predict(x_test, batch_size=batch_size)\n",
    "        #argsorted_scores= np.argsort(predicted_scores)\n",
    "        argsorted_scores = np.argpartition(np.transpose(predicted_scores)[0], 1)\n",
    "        \n",
    "        predicted_summary = []\n",
    "        summary_length = 0\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        while i < len(sentences) and summary_length < upper_bound: \n",
    "            sentence = sentences[argsorted_scores[i]]\n",
    "            #if ( dummy_rouge( sentence , predicted_summary ) < threshold ):\n",
    "            sentence = np.array([sentence])\n",
    "            #print(sentence, predicted_summary)\n",
    "            predicted_summary.append(sentence)\n",
    "            summary_length += len(nltk.word_tokenize(sentence[0]))\n",
    "                \n",
    "            i+=1\n",
    "            \n",
    "        #evals.append(dummy_rouge( predicted_summary, true_summary, alpha = N))\n",
    "        #r1score = rouge.saliency(predicted_summary, true_summary, alpha=1)\n",
    "        #r2score = rouge.saliency(predicted_summary, true_summary, alpha=0)\n",
    "\n",
    "        temp = []\n",
    "        for s in predicted_summary:\n",
    "            temp.append(s[0])\n",
    "        predicted_summary = '\\n'.join(temp)\n",
    "        \n",
    "        for s in true_summary:\n",
    "            temp.append(s)\n",
    "        #print(\"**********************\")\n",
    "        #print(predicted_summary)\n",
    "        #print(true_summary)\n",
    "        #print(\"**********************\")\n",
    "        \n",
    "        all_predicted_summary.append(predicted_summary)\n",
    "        all_true_summary.append(true_summary)\n",
    "\n",
    "        #evals.append(rouge.saliency(predicted_summary, true_summary, alpha=N))\n",
    "        summaries.append((predicted_summary, true_summary))\n",
    "\n",
    "\n",
    "    scores = rouge.evaluate_tokenized(all_predicted_summary, all_true_summary)\n",
    "    print(\" *--*--*--*--*--*--*--*\")\n",
    "\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ee97d99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 185 of 185 sentences -- 1.0 %8378378378378 %b/perdocs\r"
     ]
    }
   ],
   "source": [
    "model = load_model('../models/model-nfilt-200.h5')\n",
    "#joblib.dump(model, 'model_v1.pkl')\n",
    "#model = joblib.load('model_v1.pkl')\n",
    "#testing_data = dummy_loadTestData()\n",
    "#testing_data = loadTestData(\"../data/DUC2002_Summarization_Documents\")\n",
    "#testing_data = loadTestData(\"../data/test_subset\")\n",
    "testing_data = loadTestData(\"../data/test_smallerset\")\n",
    "#testing_data = loadTestData(\"../data/subset/data/training/d01a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0d487e94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 57 300\n",
      " *--*--*--*--*--*--*--*\n",
      "{'rouge-1': {'f': 0.26691042047531993,\n",
      "             'p': 0.15474297827239003,\n",
      "             'r': 0.9700996677740864},\n",
      " 'rouge-2': {'f': 0.23235563703024747,\n",
      "             'p': 0.13455414012738853,\n",
      "             'r': 0.8506711409395973},\n",
      " 'rouge-4': {'f': 0.11244239631336407,\n",
      "             'p': 0.06496272630457935,\n",
      "             'r': 0.4178082191780822},\n",
      " 'rouge-l': {'f': 0.26691042047531993,\n",
      "             'p': 0.15474297827239003,\n",
      "             'r': 0.9700996677740864},\n",
      " 'rouge-s4': {'f': 0.23944700460829496,\n",
      "              'p': 0.13833865814696486,\n",
      "              'r': 0.8897260273972603},\n",
      " 'rouge-su4': {'f': 0.24370779619398406,\n",
      "               'p': 0.14085506475075393,\n",
      "               'r': 0.9032992036405005},\n",
      " 'rouge-w-1.2': {'f': 0.12431536423599997,\n",
      "                 'p': 0.07214961508524717,\n",
      "                 'r': 0.4488273544373458}}\n"
     ]
    }
   ],
   "source": [
    "rouge_scores = evaluate(model, testing_data[0:1], upper_bound=100,)\n",
    "pprint(rouge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8b1ae5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1f64cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, 300)\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "print((testing_data[2][1][0]).shape)\n",
    "v = 0\n",
    "mx = 0\n",
    "for st in testing_data[2][0]:\n",
    "    #print(st)\n",
    "    #o = input()    \n",
    "    mx = max(mx, len(st.split()))\n",
    "    v += len(st.split())\n",
    "print(mx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc762877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4f1ada5",
   "metadata": {},
   "source": [
    "## Code for Article extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e08627c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSentenceEmbeddings(sentences):\n",
    "    size = len(sentences)\n",
    "\n",
    "    test_data = []\n",
    "    count = 0\n",
    "    max_size = 0\n",
    "    \n",
    "    documents_over_190 = 0\n",
    "    sentences_over_190 = 0\n",
    "    sentences_removed = 0\n",
    "    over_190 = False\n",
    "\n",
    "    arr = np.ones((len(sentences), 3), dtype=object) \n",
    "    arr[:,0] = \"dummy\"\n",
    "    arr[:,1] = np.array(sentences)\n",
    "    embedding = embed_sentences(arr)\n",
    "    embedding = embedding[0::2]\n",
    "\n",
    "    for e in embedding:\n",
    "        if len(e) > max_size:\n",
    "            max_size = len(e)\n",
    "        if len(e) > 190:\n",
    "            sentences_over_190 += 1\n",
    "            over_190 = True\n",
    "    if over_190:\n",
    "        documents_over_190 += 1\n",
    "        over_190 = False\n",
    "        count -= len(sentences)\n",
    "        sentences_removed += len(sentences)\n",
    "        return\n",
    "\n",
    "    count += len(sentences)\n",
    "    test_data.append((np.array(sentences), np.array(embedding)))\n",
    "    print(\"Finished\", count, \"of\", size,\"sentences --\", count/size,\"%\", end='\\r')\n",
    "\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "88bc8f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentences):\n",
    "    # remove punctuations, numbers and special characters\n",
    "    clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "    # make alphabets lowercase\n",
    "    clean_sentences = [s.lower() for s in clean_sentences]\n",
    "    return clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d4a14180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data, batch_size = 128, upper_bound = 100, threshold = 1):\n",
    "    \"\"\"\n",
    "        Predict the summary\n",
    "        \n",
    "        Parameters: \n",
    "            data           - np.array \n",
    "                                        ex: [ doc1, doc2, ... , docn]\n",
    "                                         where doci = [sentences, x_test]\n",
    "                                             where sentences = np.array of string\n",
    "                                                   x_test = np.array of matrices (embedded sentences)        \n",
    "        Returns: \n",
    "            Summary\n",
    "    \"\"\"   \n",
    "    \n",
    "    for doc in data: \n",
    "        sentences = doc[0]\n",
    "        \n",
    "        x_test_old = doc[1]\n",
    "        s1 = x_test_old.shape[0]\n",
    "        (s3,s4) = x_test_old[0].shape\n",
    "        print(s1,s3,s4)\n",
    "        x_test = np.random.rand(s1,1,190,s4)\n",
    "        for i in range(s1) :\n",
    "            x_test[i] = np.array( [ np.pad(x_test_old[i], ((190-s3,0),(0,0)), 'constant') ] )\n",
    "        \n",
    "        predicted_scores = model.predict(x_test, batch_size=batch_size)\n",
    "        #argsorted_scores= np.argsort(predicted_scores)\n",
    "        argsorted_scores = np.argpartition(np.transpose(predicted_scores)[0], 1)\n",
    "        \n",
    "        predicted_summary = []\n",
    "        summary_length = 0\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        while i < len(sentences) and summary_length < upper_bound: \n",
    "            sentence = sentences[argsorted_scores[i]]\n",
    "            #if ( dummy_rouge( sentence , predicted_summary ) < threshold ):\n",
    "            sentence = np.array([sentence])\n",
    "            #print(sentence, predicted_summary)\n",
    "            predicted_summary.append(sentence)\n",
    "            summary_length += len(nltk.word_tokenize(sentence[0]))\n",
    "                \n",
    "            i+=1\n",
    "\n",
    "        temp = []\n",
    "        for s in predicted_summary:\n",
    "            temp.append(s[0])\n",
    "        predicted_summary = ' '.join(temp)\n",
    "        return predicted_summary\n",
    "\n",
    "    return []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ba6927a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary(model, article, summary_length):\n",
    "    sentences = [sent_tokenize(article)]\n",
    "    sentences = [y for x in sentences for y in x]\n",
    "    clean_sentences = preprocessing(sentences)\n",
    "    #print(clean_sentences)\n",
    "    sentences_vector = createSentenceEmbeddings(clean_sentences)\n",
    "#     print(sentences_vector[0][1].shape)\n",
    "    return predict(model, sentences_vector, upper_bound=summary_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e3df5612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(model, url, summary_length):\n",
    "    \"\"\"\n",
    "        :param summary_length: length of the summary in percentage\n",
    "        :param url: url to scrape from the web\n",
    "        :return: call the summarize function\n",
    "        \"\"\"\n",
    "    # url = url.strip(\"https://\")\n",
    "    article_huff = Article(url)\n",
    "    slept = 0\n",
    "    article_huff.download()\n",
    "    while article_huff.download_state == ArticleDownloadState.NOT_STARTED:\n",
    "        # Raise exception if article download state does not change after 12 seconds\n",
    "        if slept > 13:\n",
    "            raise ArticleException('Download never started')\n",
    "        sleep(1)\n",
    "        slept += 1\n",
    "\n",
    "    article_huff.parse()\n",
    "    news_text, news_title = article_huff.text, article_huff.title\n",
    "    summary = create_summary(model, article_huff.text, summary_length)\n",
    "    return {\"title\": article_huff.title, \"summary\": summary, \"article\": news_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "811dfcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 59 300 7 of 7 sentences -- 1.0 %\n",
      "{'article': 'A conservative group is calling out Sen. Marco Rubio (R-Fla.) for '\n",
      "            'hypocrisy when it comes to former President Donald Trump.\\n'\n",
      "            '\\n'\n",
      "            'Rubio warned in 2016 that Trump “would shatter the party and the '\n",
      "            'conservative movement” and called him a “con artist.” But as the '\n",
      "            'new video from the Republican Accountability Project points out, '\n",
      "            'he’s not only turned into a Trump defender, he’s also won the '\n",
      "            'former president’s endorsement for next year’s reelection '\n",
      "            'effort.\\n'\n",
      "            '\\n'\n",
      "            '“This is an extra-special endorsement because he’s a resident and '\n",
      "            'voter in Florida,” Rubio crowed last week. “So, I don’t just need '\n",
      "            'his endorsement, I need his vote and the votes of all his family '\n",
      "            'who are also moving to Florida.”\\n'\n",
      "            '\\n'\n",
      "            'The new video slams Rubio for the flip-flop.\\n'\n",
      "            '\\n'\n",
      "            '“He’s known all along that Trump would be a unique danger to the '\n",
      "            'GOP and our country,” the voiceover said. “But that hasn’t '\n",
      "            'stopped him from supporting Trump for the past four years, and '\n",
      "            'groveling for his endorsement and vote today.”\\n'\n",
      "            '\\n'\n",
      "            'The spot will be targeted at Florida residents online and will '\n",
      "            'air on Fox News during “Hannity” in the West Palm Beach market, '\n",
      "            'which is also the home of Trump’s Mar-a-Lago residence and club.\\n'\n",
      "            '\\n'\n",
      "            'The Republican Accountability Project ― part of Defending '\n",
      "            'Democracy Together, a never-Trump conservative group ― has been '\n",
      "            'calling out Trump’s enablers in its media campaigns as well as '\n",
      "            'thanking Republicans who stood up to the former president and '\n",
      "            'voted to impeach him earlier this year.',\n",
      " 'summary': 'a conservative group is calling out sen  marco rubio  r fla   for '\n",
      "            'hypocrisy when it comes to former president donald trump   this '\n",
      "            'is an extra special endorsement because he s a resident and voter '\n",
      "            'in florida   rubio crowed last week  rubio warned in      that '\n",
      "            'trump  would shatter the party and the conservative movement  and '\n",
      "            'called him a  con artist   but as the new video from the '\n",
      "            'republican accountability project points out  he s not only '\n",
      "            'turned into a trump defender  he s also won the former president '\n",
      "            's endorsement for next year s reelection effort   so  i don t '\n",
      "            'just need his endorsement  i need his vote and the votes of all '\n",
      "            'his family who are also moving to florida    the new video slams '\n",
      "            'rubio for the flip flop ',\n",
      " 'title': 'Conservative Group Calls Out ‘Groveling’ Marco Rubio Over Trump In '\n",
      "          'Damning Ad'}\n"
     ]
    }
   ],
   "source": [
    "pprint(summarize(model, 'https://www.huffpost.com/entry/marco-rubio-donald-trump_n_607e5df5e4b063a636fb3f39', 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e538ea8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
