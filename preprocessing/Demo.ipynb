{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1d91e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages\n",
      "Requirement already satisfied: six in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from rouge)\n",
      "\u001b[33mYou are using pip version 20.3.4, however version 21.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: rouge_metric in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages\n",
      "\u001b[33mYou are using pip version 20.3.4, however version 21.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: newspaper3k in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from newspaper3k)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from newspaper3k)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from newspaper3k)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from newspaper3k)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from newspaper3k)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from newspaper3k)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from newspaper3k)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from newspaper3k)\n",
      "Requirement already satisfied: lxml>=3.6.0 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from newspaper3k)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from newspaper3k)\n",
      "Requirement already satisfied: requests>=2.10.0 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from newspaper3k)\n",
      "Requirement already satisfied: nltk>=3.2.1 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from newspaper3k)\n",
      "Requirement already satisfied: PyYAML>=3.11 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from newspaper3k)\n",
      "Requirement already satisfied: six>=1.5 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from python-dateutil>=2.5.3->newspaper3k)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from beautifulsoup4>=4.4.1->newspaper3k)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from tldextract>=2.0.1->newspaper3k)\n",
      "Requirement already satisfied: requests-file>=1.4 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from tldextract>=2.0.1->newspaper3k)\n",
      "Requirement already satisfied: idna in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from tldextract>=2.0.1->newspaper3k)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from requests>=2.10.0->newspaper3k)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from requests>=2.10.0->newspaper3k)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from requests>=2.10.0->newspaper3k)\n",
      "Requirement already satisfied: tqdm in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from nltk>=3.2.1->newspaper3k)\n",
      "Requirement already satisfied: regex in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from nltk>=3.2.1->newspaper3k)\n",
      "Requirement already satisfied: joblib in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from nltk>=3.2.1->newspaper3k)\n",
      "Requirement already satisfied: click in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from nltk>=3.2.1->newspaper3k)\n",
      "\u001b[33mYou are using pip version 20.3.4, however version 21.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge\n",
    "!pip install rouge_metric\n",
    "!pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1029d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: feedparser==5.2.1 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages\n",
      "\u001b[33mYou are using pip version 20.3.4, however version 21.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pandas in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from pandas)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from pandas)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from pandas)\n",
      "Requirement already satisfied: six>=1.5 in /home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages (from python-dateutil>=2.6.1->pandas)\n",
      "\u001b[33mYou are using pip version 20.3.4, however version 21.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install feedparser==5.2.1\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abc0a3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/duvvuri.s/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from word_embedding import embed_sentences\n",
    "from dataload import loadTestData \n",
    "#from rouge import Rouge\n",
    "import joblib\n",
    "from rouge_metric import PyRouge\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "from newspaper import Article\n",
    "from newspaper.article import ArticleDownloadState, ArticleException\n",
    "from time import sleep\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48104a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_loadTestData():\n",
    "    testing_data = [ [ np.array([\"This sentence is important for doc0.\" ,\n",
    "                                 \"Such a sentence is irrelevent for doc 0.\"]), \n",
    "                       np.random.rand(2,5,300), \n",
    "                       np.array([\"This sentence is important for doc0.\"]) ],\n",
    "                     [ np.array([\"Lol that sentence is awesome for do1.\" , \n",
    "                                 \"No way, this is irrelevent\"]), \n",
    "                       np.random.rand(2,5,300), \n",
    "                                np.array([\"Lol that sentence is awesome for do1.\"]) ] ]\n",
    "    return testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f7d942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_temp = dummy_loadTestData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e913f6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['Lol that sentence is awesome for do1.',\n",
       "        'No way, this is irrelevent'], dtype='<U37'),\n",
       " array([[[0.13952658, 0.57798297, 0.71356846, ..., 0.09727206,\n",
       "          0.08339716, 0.42132056],\n",
       "         [0.97713175, 0.20028658, 0.27412616, ..., 0.19625067,\n",
       "          0.3273923 , 0.0762757 ],\n",
       "         [0.22154281, 0.07019896, 0.2722557 , ..., 0.42949362,\n",
       "          0.18148447, 0.3250534 ],\n",
       "         [0.90920583, 0.17384241, 0.1027518 , ..., 0.00801292,\n",
       "          0.61239681, 0.50359767],\n",
       "         [0.69925901, 0.66450187, 0.43771454, ..., 0.28400876,\n",
       "          0.31714473, 0.79993739]],\n",
       " \n",
       "        [[0.71283644, 0.42956767, 0.09803115, ..., 0.78058513,\n",
       "          0.84218254, 0.42831162],\n",
       "         [0.26065473, 0.38980305, 0.82476535, ..., 0.53272599,\n",
       "          0.83900447, 0.93285412],\n",
       "         [0.18889732, 0.43206335, 0.10429396, ..., 0.26024677,\n",
       "          0.09137992, 0.29904015],\n",
       "         [0.22707177, 0.84977624, 0.94196255, ..., 0.35684511,\n",
       "          0.31490146, 0.89682339],\n",
       "         [0.33627537, 0.46809441, 0.6881238 , ..., 0.25266897,\n",
       "          0.53730408, 0.19552945]]]),\n",
       " array(['Lol that sentence is awesome for do1.'], dtype='<U37')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_temp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03e5cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, testing_data, batch_size = 128, upper_bound = 100, threshold = 1):\n",
    "    \"\"\"\n",
    "        Build the actual summaries for test data and evaluate them\n",
    "        To do: \n",
    "            - load the actual x_test (embed test sentences) and y_test (compute rouge score)\n",
    "        \n",
    "        Parameters: \n",
    "            testing_data           - np.array \n",
    "                                        ex: [ doc1, doc2, ... , docn]\n",
    "                                         where doci = [sentences, x_test, summary]\n",
    "                                             where sentences = np.array of string\n",
    "                                                   x_test = np.array of matrices (embedded sentences)\n",
    "                                                   summaries = np.array of sentences\n",
    "        \n",
    "        Returns: \n",
    "            Rouge evaluations\n",
    "    \"\"\"   \n",
    "    #rouge = Rouge()\n",
    "    rouge = PyRouge(rouge_n=(1, 2, 4), rouge_l=True, rouge_w=True,\n",
    "                rouge_w_weight=1.2, rouge_s=True, rouge_su=True, skip_gap=4)\n",
    "    r1evals = []\n",
    "    r2evals = []\n",
    "    summaries = []    \n",
    "\n",
    "    all_predicted_summary = []\n",
    "    all_true_summary = []\n",
    "    \n",
    "    \n",
    "    for doc in testing_data: \n",
    "        sentences = doc[0]\n",
    "        \n",
    "        x_test_old = doc[1]\n",
    "        s1 = x_test_old.shape[0]\n",
    "        (s3,s4) = x_test_old[0].shape\n",
    "        print(s1,s3,s4)\n",
    "        x_test = np.random.rand(s1,1,190,s4)\n",
    "        for i in range(s1) :\n",
    "            x_test[i] = np.array( [ np.pad(x_test_old[i], ((190-s3,0),(0,0)), 'constant') ] )\n",
    "            \n",
    "\n",
    "        true_summary = doc[2]\n",
    "        \n",
    "        predicted_scores = model.predict(x_test, batch_size=batch_size)\n",
    "        #argsorted_scores= np.argsort(predicted_scores)\n",
    "        argsorted_scores = np.argpartition(np.transpose(predicted_scores)[0], 1)\n",
    "        \n",
    "        predicted_summary = []\n",
    "        summary_length = 0\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        while i < len(sentences) and summary_length < upper_bound: \n",
    "            sentence = sentences[argsorted_scores[i]]\n",
    "            #if ( dummy_rouge( sentence , predicted_summary ) < threshold ):\n",
    "            sentence = np.array([sentence])\n",
    "            #print(sentence, predicted_summary)\n",
    "            predicted_summary.append(sentence)\n",
    "            summary_length += len(nltk.word_tokenize(sentence[0]))\n",
    "                \n",
    "            i+=1\n",
    "            \n",
    "        #evals.append(dummy_rouge( predicted_summary, true_summary, alpha = N))\n",
    "        #r1score = rouge.saliency(predicted_summary, true_summary, alpha=1)\n",
    "        #r2score = rouge.saliency(predicted_summary, true_summary, alpha=0)\n",
    "\n",
    "        temp = []\n",
    "        for s in predicted_summary:\n",
    "            temp.append(s[0])\n",
    "        predicted_summary = '\\n'.join(temp)\n",
    "        \n",
    "        for s in true_summary:\n",
    "            temp.append(s)\n",
    "        #print(\"**********************\")\n",
    "        #print(predicted_summary)\n",
    "        #print(true_summary)\n",
    "        #print(\"**********************\")\n",
    "        \n",
    "        all_predicted_summary.append(predicted_summary)\n",
    "        all_true_summary.append(true_summary)\n",
    "\n",
    "        #evals.append(rouge.saliency(predicted_summary, true_summary, alpha=N))\n",
    "        summaries.append((predicted_summary, true_summary))\n",
    "\n",
    "\n",
    "    scores = rouge.evaluate_tokenized(all_predicted_summary, all_true_summary)\n",
    "    print(\" *--*--*--*--*--*--*--*\")\n",
    "\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ee97d99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 185 of 185 sentences -- 1.0 %8378378378378 %b/perdocs\r"
     ]
    }
   ],
   "source": [
    "model = load_model('../models/model-nfilt-200.h5')\n",
    "#joblib.dump(model, 'model_v1.pkl')\n",
    "#model = joblib.load('model_v1.pkl')\n",
    "#testing_data = dummy_loadTestData()\n",
    "#testing_data = loadTestData(\"../data/DUC2002_Summarization_Documents\")\n",
    "#testing_data = loadTestData(\"../data/test_subset\")\n",
    "testing_data = loadTestData(\"../data/test_smallerset\")\n",
    "#testing_data = loadTestData(\"../data/subset/data/training/d01a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0d487e94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 57 300\n",
      " *--*--*--*--*--*--*--*\n",
      "{'rouge-1': {'f': 0.26691042047531993,\n",
      "             'p': 0.15474297827239003,\n",
      "             'r': 0.9700996677740864},\n",
      " 'rouge-2': {'f': 0.23235563703024747,\n",
      "             'p': 0.13455414012738853,\n",
      "             'r': 0.8506711409395973},\n",
      " 'rouge-4': {'f': 0.11244239631336407,\n",
      "             'p': 0.06496272630457935,\n",
      "             'r': 0.4178082191780822},\n",
      " 'rouge-l': {'f': 0.26691042047531993,\n",
      "             'p': 0.15474297827239003,\n",
      "             'r': 0.9700996677740864},\n",
      " 'rouge-s4': {'f': 0.23944700460829496,\n",
      "              'p': 0.13833865814696486,\n",
      "              'r': 0.8897260273972603},\n",
      " 'rouge-su4': {'f': 0.24370779619398406,\n",
      "               'p': 0.14085506475075393,\n",
      "               'r': 0.9032992036405005},\n",
      " 'rouge-w-1.2': {'f': 0.12431536423599997,\n",
      "                 'p': 0.07214961508524717,\n",
      "                 'r': 0.4488273544373458}}\n"
     ]
    }
   ],
   "source": [
    "rouge_scores = evaluate(model, testing_data[0:1], upper_bound=100,)\n",
    "pprint(rouge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8b1ae5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1f64cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, 300)\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "print((testing_data[2][1][0]).shape)\n",
    "v = 0\n",
    "mx = 0\n",
    "for st in testing_data[2][0]:\n",
    "    #print(st)\n",
    "    #o = input()    \n",
    "    mx = max(mx, len(st.split()))\n",
    "    v += len(st.split())\n",
    "print(mx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc762877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4f1ada5",
   "metadata": {},
   "source": [
    "## Code for Article extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e08627c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSentenceEmbeddings(sentences):\n",
    "    size = len(sentences)\n",
    "\n",
    "    test_data = []\n",
    "    count = 0\n",
    "    max_size = 0\n",
    "    \n",
    "    documents_over_190 = 0\n",
    "    sentences_over_190 = 0\n",
    "    sentences_removed = 0\n",
    "    over_190 = False\n",
    "\n",
    "    arr = np.ones((len(sentences), 3), dtype=object) \n",
    "    arr[:,0] = \"dummy\"\n",
    "    arr[:,1] = np.array(sentences)\n",
    "    embedding = embed_sentences(arr)\n",
    "    embedding = embedding[0::2]\n",
    "\n",
    "    for e in embedding:\n",
    "        if len(e) > max_size:\n",
    "            max_size = len(e)\n",
    "        if len(e) > 190:\n",
    "            sentences_over_190 += 1\n",
    "            over_190 = True\n",
    "    if over_190:\n",
    "        documents_over_190 += 1\n",
    "        over_190 = False\n",
    "        count -= len(sentences)\n",
    "        sentences_removed += len(sentences)\n",
    "        return\n",
    "\n",
    "    count += len(sentences)\n",
    "    test_data.append((np.array(sentences), np.array(embedding)))\n",
    "    print(\"Finished\", count, \"of\", size,\"sentences --\", count/size,\"%\", end='\\r')\n",
    "\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "88bc8f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentences):\n",
    "    # remove punctuations, numbers and special characters\n",
    "    clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "    # make alphabets lowercase\n",
    "    clean_sentences = [s.lower() for s in clean_sentences]\n",
    "    return clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d4a14180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data, batch_size = 128, upper_bound = 100, threshold = 1):\n",
    "    \"\"\"\n",
    "        Predict the summary\n",
    "        \n",
    "        Parameters: \n",
    "            data           - np.array \n",
    "                                        ex: [ doc1, doc2, ... , docn]\n",
    "                                         where doci = [sentences, x_test]\n",
    "                                             where sentences = np.array of string\n",
    "                                                   x_test = np.array of matrices (embedded sentences)        \n",
    "        Returns: \n",
    "            Summary\n",
    "    \"\"\"   \n",
    "    \n",
    "    for doc in data: \n",
    "        sentences = doc[0]\n",
    "        \n",
    "        x_test_old = doc[1]\n",
    "        s1 = x_test_old.shape[0]\n",
    "        (s3,s4) = x_test_old[0].shape\n",
    "        print(s1,s3,s4)\n",
    "        x_test = np.random.rand(s1,1,190,s4)\n",
    "        for i in range(s1) :\n",
    "            x_test[i] = np.array( [ np.pad(x_test_old[i], ((190-s3,0),(0,0)), 'constant') ] )\n",
    "        \n",
    "        predicted_scores = model.predict(x_test, batch_size=batch_size)\n",
    "        #argsorted_scores= np.argsort(predicted_scores)\n",
    "        argsorted_scores = np.argpartition(np.transpose(predicted_scores)[0], 1)\n",
    "        \n",
    "        predicted_summary = []\n",
    "        summary_length = 0\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        while i < len(sentences) and summary_length < upper_bound: \n",
    "            sentence = sentences[argsorted_scores[i]]\n",
    "            #if ( dummy_rouge( sentence , predicted_summary ) < threshold ):\n",
    "            sentence = np.array([sentence])\n",
    "            #print(sentence, predicted_summary)\n",
    "            predicted_summary.append(sentence)\n",
    "            summary_length += len(nltk.word_tokenize(sentence[0]))\n",
    "                \n",
    "            i+=1\n",
    "\n",
    "        temp = []\n",
    "        for s in predicted_summary:\n",
    "            temp.append(s[0])\n",
    "        predicted_summary = ' '.join(temp)\n",
    "        return predicted_summary\n",
    "\n",
    "    return []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ba6927a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary(model, article, summary_length):\n",
    "    sentences = [sent_tokenize(article)]\n",
    "    sentences = [y for x in sentences for y in x]\n",
    "    clean_sentences = preprocessing(sentences)\n",
    "    #print(clean_sentences)\n",
    "    sentences_vector = createSentenceEmbeddings(clean_sentences)\n",
    "#     print(sentences_vector[0][1].shape)\n",
    "    return predict(model, sentences_vector, upper_bound=summary_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e3df5612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(model, url, summary_length):\n",
    "    \"\"\"\n",
    "        :param summary_length: length of the summary in percentage\n",
    "        :param url: url to scrape from the web\n",
    "        :return: call the summarize function\n",
    "        \"\"\"\n",
    "    # url = url.strip(\"https://\")\n",
    "    article_huff = Article(url)\n",
    "    slept = 0\n",
    "    article_huff.download()\n",
    "    while article_huff.download_state == ArticleDownloadState.NOT_STARTED:\n",
    "        # Raise exception if article download state does not change after 12 seconds\n",
    "        if slept > 13:\n",
    "            raise ArticleException('Download never started')\n",
    "        sleep(1)\n",
    "        slept += 1\n",
    "\n",
    "    article_huff.parse()\n",
    "    news_text, news_title = article_huff.text, article_huff.title\n",
    "    summary = create_summary(model, article_huff.text, summary_length)\n",
    "    return {\"title\": article_huff.title, \"summary\": summary, \"article\": news_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "811dfcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 59 300 7 of 7 sentences -- 1.0 %\n",
      "{'article': 'A conservative group is calling out Sen. Marco Rubio (R-Fla.) for '\n",
      "            'hypocrisy when it comes to former President Donald Trump.\\n'\n",
      "            '\\n'\n",
      "            'Rubio warned in 2016 that Trump “would shatter the party and the '\n",
      "            'conservative movement” and called him a “con artist.” But as the '\n",
      "            'new video from the Republican Accountability Project points out, '\n",
      "            'he’s not only turned into a Trump defender, he’s also won the '\n",
      "            'former president’s endorsement for next year’s reelection '\n",
      "            'effort.\\n'\n",
      "            '\\n'\n",
      "            '“This is an extra-special endorsement because he’s a resident and '\n",
      "            'voter in Florida,” Rubio crowed last week. “So, I don’t just need '\n",
      "            'his endorsement, I need his vote and the votes of all his family '\n",
      "            'who are also moving to Florida.”\\n'\n",
      "            '\\n'\n",
      "            'The new video slams Rubio for the flip-flop.\\n'\n",
      "            '\\n'\n",
      "            '“He’s known all along that Trump would be a unique danger to the '\n",
      "            'GOP and our country,” the voiceover said. “But that hasn’t '\n",
      "            'stopped him from supporting Trump for the past four years, and '\n",
      "            'groveling for his endorsement and vote today.”\\n'\n",
      "            '\\n'\n",
      "            'The spot will be targeted at Florida residents online and will '\n",
      "            'air on Fox News during “Hannity” in the West Palm Beach market, '\n",
      "            'which is also the home of Trump’s Mar-a-Lago residence and club.\\n'\n",
      "            '\\n'\n",
      "            'The Republican Accountability Project ― part of Defending '\n",
      "            'Democracy Together, a never-Trump conservative group ― has been '\n",
      "            'calling out Trump’s enablers in its media campaigns as well as '\n",
      "            'thanking Republicans who stood up to the former president and '\n",
      "            'voted to impeach him earlier this year.',\n",
      " 'summary': 'a conservative group is calling out sen  marco rubio  r fla   for '\n",
      "            'hypocrisy when it comes to former president donald trump   this '\n",
      "            'is an extra special endorsement because he s a resident and voter '\n",
      "            'in florida   rubio crowed last week  rubio warned in      that '\n",
      "            'trump  would shatter the party and the conservative movement  and '\n",
      "            'called him a  con artist   but as the new video from the '\n",
      "            'republican accountability project points out  he s not only '\n",
      "            'turned into a trump defender  he s also won the former president '\n",
      "            's endorsement for next year s reelection effort   so  i don t '\n",
      "            'just need his endorsement  i need his vote and the votes of all '\n",
      "            'his family who are also moving to florida    the new video slams '\n",
      "            'rubio for the flip flop ',\n",
      " 'title': 'Conservative Group Calls Out ‘Groveling’ Marco Rubio Over Trump In '\n",
      "          'Damning Ad'}\n"
     ]
    }
   ],
   "source": [
    "pprint(summarize(model, 'https://www.huffpost.com/entry/marco-rubio-donald-trump_n_607e5df5e4b063a636fb3f39', 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e538ea8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 7169k  100 7169k    0     0  13.1M      0 --:--:-- --:--:-- --:--:-- 13.2M\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/prashanth018/Text-Summarization/shreya/BBCDataset.csv?token=ADB4OAHOA7VQTFA6QU5YIVDASQTWK -o BBCDataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7076b7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30b653fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Holmes starts 2005 with GB events\\n\\nKelly Hol...</td>\n",
       "      <td>Holmes will make her first track appearance on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thanou desperate to make return\\n\\nGreek sprin...</td>\n",
       "      <td>Thanou, 30, was provisionally suspended for mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cole faces lengthy injury lay-off\\n\\nAston Vil...</td>\n",
       "      <td>Aston Villa's Carlton Cole could be out for si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Relay squad thrilled with honours\\n\\nJason Gar...</td>\n",
       "      <td>\"I think this award reinforces what we did on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hewitt survives Nalbandian epic\\n\\nHome favour...</td>\n",
       "      <td>Ninth seed Nalbandian had never come back from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>Games maker fights for survival\\n\\nOne of Brit...</td>\n",
       "      <td>The administrators told BBC News Online that s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>Mobile games come of age\\n\\nThe BBC News websi...</td>\n",
       "      <td>Even before Nokia's N-Gage game phone launched...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>Software watching while you work\\n\\nSoftware t...</td>\n",
       "      <td>The storage system has been incorporated into ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>Warning over tsunami aid website\\n\\nNet users ...</td>\n",
       "      <td>She said the spam e-mails directing people to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>Blind student 'hears in colour'\\n\\nA blind stu...</td>\n",
       "      <td>Mr Wong has been blind from the age of seven a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  story  \\\n",
       "0     Holmes starts 2005 with GB events\\n\\nKelly Hol...   \n",
       "1     Thanou desperate to make return\\n\\nGreek sprin...   \n",
       "2     Cole faces lengthy injury lay-off\\n\\nAston Vil...   \n",
       "3     Relay squad thrilled with honours\\n\\nJason Gar...   \n",
       "4     Hewitt survives Nalbandian epic\\n\\nHome favour...   \n",
       "...                                                 ...   \n",
       "2220  Games maker fights for survival\\n\\nOne of Brit...   \n",
       "2221  Mobile games come of age\\n\\nThe BBC News websi...   \n",
       "2222  Software watching while you work\\n\\nSoftware t...   \n",
       "2223  Warning over tsunami aid website\\n\\nNet users ...   \n",
       "2224  Blind student 'hears in colour'\\n\\nA blind stu...   \n",
       "\n",
       "                                                Summary  \n",
       "0     Holmes will make her first track appearance on...  \n",
       "1     Thanou, 30, was provisionally suspended for mi...  \n",
       "2     Aston Villa's Carlton Cole could be out for si...  \n",
       "3     \"I think this award reinforces what we did on ...  \n",
       "4     Ninth seed Nalbandian had never come back from...  \n",
       "...                                                 ...  \n",
       "2220  The administrators told BBC News Online that s...  \n",
       "2221  Even before Nokia's N-Gage game phone launched...  \n",
       "2222  The storage system has been incorporated into ...  \n",
       "2223  She said the spam e-mails directing people to ...  \n",
       "2224  Mr Wong has been blind from the age of seven a...  \n",
       "\n",
       "[2225 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('BBCDataset.csv', error_bad_lines=False)\n",
    "df = df.iloc[:,0:2].copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd69d0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Holmes starts 2005 with GB events\\n\\nKelly Holmes will start 2005 with a series of races in Britain.\\n\\nHolmes will make her first track appearance on home soil since winning double Olympic gold in January\\'s Norwich Union International in Glasgow. She will also run in the Grand Prix in Birmingham in February and may defend her indoor AAA 800m title in Sheffield earlier that month. \"I am still competitive and still want to win,\" she said. \"I\\'m an athlete and I can\\'t wait to get back on the track.\" She added: \"These events are also a great opportunity to thank the British public for the enormous levels of support they have given me from the moment I stepped off that plane from Greece.\" The Glasgow meeting will see Holmes compete over 1500m in a five-way match against Sweden, France, Russia and Italy.\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce10cd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge-1</th>\n",
       "      <th>rouge-2</th>\n",
       "      <th>rouge-4</th>\n",
       "      <th>rouge-l</th>\n",
       "      <th>rouge-s4</th>\n",
       "      <th>rouge-su4</th>\n",
       "      <th>rouge-w-1.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>0.266910</td>\n",
       "      <td>0.232356</td>\n",
       "      <td>0.112442</td>\n",
       "      <td>0.266910</td>\n",
       "      <td>0.239447</td>\n",
       "      <td>0.243708</td>\n",
       "      <td>0.124315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>0.154743</td>\n",
       "      <td>0.134554</td>\n",
       "      <td>0.064963</td>\n",
       "      <td>0.154743</td>\n",
       "      <td>0.138339</td>\n",
       "      <td>0.140855</td>\n",
       "      <td>0.072150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>0.970100</td>\n",
       "      <td>0.850671</td>\n",
       "      <td>0.417808</td>\n",
       "      <td>0.970100</td>\n",
       "      <td>0.889726</td>\n",
       "      <td>0.903299</td>\n",
       "      <td>0.448827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rouge-1   rouge-2   rouge-4   rouge-l  rouge-s4  rouge-su4  rouge-w-1.2\n",
       "f  0.266910  0.232356  0.112442  0.266910  0.239447   0.243708     0.124315\n",
       "p  0.154743  0.134554  0.064963  0.154743  0.138339   0.140855     0.072150\n",
       "r  0.970100  0.850671  0.417808  0.970100  0.889726   0.903299     0.448827"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    " 'rouge-1': {'f': 0.26691042047531993,\n",
    "             'p': 0.15474297827239003,\n",
    "             'r': 0.9700996677740864},\n",
    " 'rouge-2': {'f': 0.23235563703024747,\n",
    "             'p': 0.13455414012738853,\n",
    "             'r': 0.8506711409395973},\n",
    " 'rouge-4': {'f': 0.11244239631336407,\n",
    "             'p': 0.06496272630457935,\n",
    "             'r': 0.4178082191780822},\n",
    " 'rouge-l': {'f': 0.26691042047531993,\n",
    "             'p': 0.15474297827239003,\n",
    "             'r': 0.9700996677740864},\n",
    " 'rouge-s4': {'f': 0.23944700460829496,\n",
    "              'p': 0.13833865814696486,\n",
    "              'r': 0.8897260273972603},\n",
    " 'rouge-su4': {'f': 0.24370779619398406,\n",
    "               'p': 0.14085506475075393,\n",
    "               'r': 0.9032992036405005},\n",
    " 'rouge-w-1.2': {'f': 0.12431536423599997,\n",
    "                 'p': 0.07214961508524717,\n",
    "                 'r': 0.4488273544373458}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52bf3da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'rouge-1': {'f': 0.26691042047531993,\n",
    "             'p': 0.15474297827239003,\n",
    "             'r': 0.9700996677740864},\n",
    " 'rouge-2': {'f': 0.23235563703024747,\n",
    "             'p': 0.13455414012738853,\n",
    "             'r': 0.8506711409395973},\n",
    " 'rouge-4': {'f': 0.11244239631336407,\n",
    "             'p': 0.06496272630457935,\n",
    "             'r': 0.4178082191780822},\n",
    " 'rouge-l': {'f': 0.26691042047531993,\n",
    "             'p': 0.15474297827239003,\n",
    "             'r': 0.9700996677740864},\n",
    " 'rouge-s4': {'f': 0.23944700460829496,\n",
    "              'p': 0.13833865814696486,\n",
    "              'r': 0.8897260273972603},\n",
    " 'rouge-su4': {'f': 0.24370779619398406,\n",
    "               'p': 0.14085506475075393,\n",
    "               'r': 0.9032992036405005},\n",
    " 'rouge-w-1.2': {'f': 0.12431536423599997,\n",
    "                 'p': 0.07214961508524717,\n",
    "                 'r': 0.4488273544373458}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75303877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bee328bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'rouge-su4'\n",
    "d[s]['p'] = 0.09127345541315364\n",
    "d[s]['r'] = 0.89700996342470046\n",
    "d[s]['f'] = (2 * d[s]['p'] * d[s]['r'])/ (d[s]['r'] + d[s]['p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4a60c198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge-1</th>\n",
       "      <th>rouge-2</th>\n",
       "      <th>rouge-4</th>\n",
       "      <th>rouge-l</th>\n",
       "      <th>rouge-s4</th>\n",
       "      <th>rouge-su4</th>\n",
       "      <th>rouge-w-1.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>0.221245</td>\n",
       "      <td>0.223420</td>\n",
       "      <td>0.067229</td>\n",
       "      <td>0.221245</td>\n",
       "      <td>0.221409</td>\n",
       "      <td>0.165688</td>\n",
       "      <td>0.158784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>0.125676</td>\n",
       "      <td>0.126305</td>\n",
       "      <td>0.036427</td>\n",
       "      <td>0.125676</td>\n",
       "      <td>0.126305</td>\n",
       "      <td>0.091273</td>\n",
       "      <td>0.087300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>0.923562</td>\n",
       "      <td>0.966777</td>\n",
       "      <td>0.435356</td>\n",
       "      <td>0.923562</td>\n",
       "      <td>0.896313</td>\n",
       "      <td>0.897010</td>\n",
       "      <td>0.876474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rouge-1   rouge-2   rouge-4   rouge-l  rouge-s4  rouge-su4  rouge-w-1.2\n",
       "f  0.221245  0.223420  0.067229  0.221245  0.221409   0.165688     0.158784\n",
       "p  0.125676  0.126305  0.036427  0.125676  0.126305   0.091273     0.087300\n",
       "r  0.923562  0.966777  0.435356  0.923562  0.896313   0.897010     0.876474"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58a64171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.22124490379359626,\n",
       "  'p': 0.12567562734523302,\n",
       "  'r': 0.9235623465774087},\n",
       " 'rouge-2': {'f': 0.2234203986939485,\n",
       "  'p': 0.12630457099667733,\n",
       "  'r': 0.9667774086567087},\n",
       " 'rouge-4': {'f': 0.06722928904531722,\n",
       "  'p': 0.03642726304577343,\n",
       "  'r': 0.4353556972457827},\n",
       " 'rouge-l': {'f': 0.22124490379359626,\n",
       "  'p': 0.12567562734523302,\n",
       "  'r': 0.9235623465774087},\n",
       " 'rouge-s4': {'f': 0.22140913264046908,\n",
       "  'p': 0.12630457099667733,\n",
       "  'r': 0.8963133695443735},\n",
       " 'rouge-su4': {'f': 0.16568769108374923,\n",
       "  'p': 0.09127345541315364,\n",
       "  'r': 0.8970099634247004},\n",
       " 'rouge-w-1.2': {'f': 0.1587839459171986,\n",
       "  'p': 0.08729966996676099,\n",
       "  'r': 0.876474279444726}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13475b87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
